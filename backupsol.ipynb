{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This file provides starter code for extracting features from the xml files and\n",
    "## for doing some learning.\n",
    "##\n",
    "## The basic set-up: \n",
    "## ----------------\n",
    "## main() will run code to extract features, learn, and make predictions.\n",
    "## \n",
    "## extract_feats() is called by main(), and it will iterate through the \n",
    "## train/test directories and parse each xml file into an xml.etree.ElementTree, \n",
    "## which is a standard python object used to represent an xml file in memory.\n",
    "## (More information about xml.etree.ElementTree objects can be found here:\n",
    "## http://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "## and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)\n",
    "## It will then use a series of \"feature-functions\" that you will write/modify\n",
    "## in order to extract dictionaries of features from each ElementTree object.\n",
    "## Finally, it will produce an N x D sparse design matrix containing the union\n",
    "## of the features contained in the dictionaries produced by your \"feature-functions.\"\n",
    "## This matrix can then be plugged into your learning algorithm.\n",
    "##\n",
    "## The learning and prediction parts of main() are largely left to you, though\n",
    "## it does contain code that randomly picks class-specific weights and predicts\n",
    "## the class with the weights that give the highest score. If your prediction\n",
    "## algorithm involves class-specific weights, you should, of course, learn \n",
    "## these class-specific weights in a more intelligent way.\n",
    "##\n",
    "## Feature-functions:\n",
    "## --------------------\n",
    "## \"feature-functions\" are functions that take an ElementTree object representing\n",
    "## an xml file (which contains, among other things, the sequence of system calls a\n",
    "## piece of potential malware has made), and returns a dictionary mapping feature names to \n",
    "## their respective numeric values. \n",
    "## For instance, a simple feature-function might map a system call history to the\n",
    "## dictionary {'first_call-load_image': 1}. This is a boolean feature indicating\n",
    "## whether the first system call made by the executable was 'load_image'. \n",
    "## Real-valued or count-based features can of course also be defined in this way. \n",
    "## Because this feature-function will be run over ElementTree objects for each \n",
    "## software execution history instance, we will have the (different)\n",
    "## feature values of this feature for each history, and these values will make up \n",
    "## one of the columns in our final design matrix.\n",
    "## Of course, multiple features can be defined within a single dictionary, and in\n",
    "## the end all the dictionaries returned by feature functions (for a particular\n",
    "## training example) will be unioned, so we can collect all the feature values \n",
    "## associated with that particular instance.\n",
    "##\n",
    "## Two example feature-functions, first_last_system_call_feats() and \n",
    "## system_call_count_feats(), are defined below.\n",
    "## The first of these functions indicates what the first and last system-calls \n",
    "## made by an executable are, and the second records the total number of system\n",
    "## calls made by an executable.\n",
    "##\n",
    "## What you need to do:\n",
    "## --------------------\n",
    "## 1. Write new feature-functions (or modify the example feature-functions) to\n",
    "## extract useful features for this prediction task.\n",
    "## 2. Implement an algorithm to learn from the design matrix produced, and to\n",
    "## make predictions on unseen data. Naive code for these two steps is provided\n",
    "## below, and marked by TODOs.\n",
    "##\n",
    "## Computational Caveat\n",
    "## --------------------\n",
    "## Because the biggest of any of the xml files is only around 35MB, the code below \n",
    "## will parse an entire xml file and store it in memory, compute features, and\n",
    "## then get rid of it before parsing the next one. Storing the biggest of the files \n",
    "## in memory should require at most 200MB or so, which should be no problem for\n",
    "## reasonably modern laptops. If this is too much, however, you can lower the\n",
    "## memory requirement by using ElementTree.iterparse(), which does parsing in\n",
    "## a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\n",
    "## for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import preprocessing\n",
    "# import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "    returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    fds = [] # list of feature dicts\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        # accumulate features\n",
    "        [rowfd.update(ff(tree)) for ff in ffs]\n",
    "        fds.append(rowfd)\n",
    "        \n",
    "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "    return X, feat_dict, np.array(classes), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      fds is a list of feature dicts (one for each row).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "       \n",
    "    returns: \n",
    "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "        the union of features defined in any of the fds \n",
    "    \"\"\"\n",
    "    if global_feat_dict is None:\n",
    "        all_feats = set()\n",
    "        [all_feats.update(fd.keys()) for fd in fds]\n",
    "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "    else:\n",
    "        feat_dict = global_feat_dict\n",
    "        \n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []        \n",
    "    for i in xrange(len(fds)):\n",
    "        temp_cols = []\n",
    "        temp_data = []\n",
    "        for feat,val in fds[i].iteritems():\n",
    "            try:\n",
    "                # update temp_cols iff update temp_data\n",
    "                temp_cols.append(feat_dict[feat])\n",
    "                temp_data.append(val)\n",
    "            except KeyError as ex:\n",
    "                if global_feat_dict is not None:\n",
    "                    pass  # new feature in test data; nbd\n",
    "                else:\n",
    "                    raise ex\n",
    "\n",
    "        # all fd's features in the same row\n",
    "        k = len(temp_cols)\n",
    "        cols.extend(temp_cols)\n",
    "        data.extend(temp_data)\n",
    "        rows.extend([i]*k)\n",
    "\n",
    "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "\n",
    "    X = sparse.csr_matrix((np.array(data),\n",
    "                   (np.array(rows), np.array(cols))),\n",
    "                   shape=(len(fds), len(feat_dict)))\n",
    "    return X, feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features based on name of system calls\n",
    "def tag_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping el.tag to the number of system_calls\n",
    "      identified by that tag (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns true if a string contains any numerical characters\n",
    "def hasNumbers(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "# Extract features based on non-numerical attribute values\n",
    "def attrib_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping val to the number of keys having\n",
    "      that value in the attributes (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            attrib_dict = el.attrib\n",
    "            for key, val in attrib_dict.iteritems():\n",
    "                if not hasNumbers(val):\n",
    "                    c[val] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# # (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# # feature-names to numeric values.\n",
    "# ## TODO: modify these functions, and/or add new ones.\n",
    "# def first_last_system_call_feats(tree):\n",
    "#     \"\"\"\n",
    "#     arguments:\n",
    "#       tree is an xml.etree.ElementTree object\n",
    "#     returns:\n",
    "#       a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "#       made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "#       (in other words, it returns a dictionary indicating what the first and \n",
    "#       last system calls made by an executable were.)\n",
    "#     \"\"\"\n",
    "#     c = Counter()\n",
    "#     in_all_section = False\n",
    "#     first = True # is this the first system call\n",
    "#     last_call = None # keep track of last call we've seen\n",
    "#     for el in tree.iter():\n",
    "#         # ignore everything outside the \"all_section\" element\n",
    "#         if el.tag == \"all_section\" and not in_all_section:\n",
    "#             in_all_section = True\n",
    "#         elif el.tag == \"all_section\" and in_all_section:\n",
    "#             in_all_section = False\n",
    "#         elif in_all_section:\n",
    "#             if first:\n",
    "#                 c[\"first_call-\"+el.tag] = 1\n",
    "#                 first = False\n",
    "#             last_call = el.tag  # update last call seen\n",
    "            \n",
    "#     # finally, mark last call seen\n",
    "#     c[\"last_call-\"+last_call] = 1\n",
    "#     return c\n",
    "\n",
    "# def system_call_count_feats(tree):\n",
    "#     \"\"\"\n",
    "#     arguments:\n",
    "#       tree is an xml.etree.ElementTree object\n",
    "#     returns:\n",
    "#       a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "#       made by an executable (summed over all processes)\n",
    "#     \"\"\"\n",
    "#     c = Counter()\n",
    "#     in_all_section = False\n",
    "#     for el in tree.iter():\n",
    "#         # ignore everything outside the \"all_section\" element\n",
    "#         if el.tag == \"all_section\" and not in_all_section:\n",
    "#             in_all_section = True\n",
    "#         elif el.tag == \"all_section\" and in_all_section:\n",
    "#             in_all_section = False\n",
    "#         elif in_all_section:\n",
    "#             c['num_system_calls'] += 1\n",
    "#     return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Extract features based on system call proportions\n",
    "# def tag_prop_feats(tree):\n",
    "#     \"\"\"\n",
    "#     arguments:\n",
    "#       tree is an xml.etree.ElementTree object\n",
    "#     returns:\n",
    "#       a dictionary mapping el.tag to the number of system_calls\n",
    "#       identified by that tag (summed over all processes)\n",
    "#     \"\"\"\n",
    "#     c = Counter()\n",
    "#     in_all_section = False\n",
    "#     for el in tree.iter():\n",
    "#         # ignore everything outside the \"all_section\" element\n",
    "#         if el.tag == \"all_section\" and not in_all_section:\n",
    "#             in_all_section = True\n",
    "#         elif el.tag == \"all_section\" and in_all_section:\n",
    "#             in_all_section = False\n",
    "#         elif in_all_section:\n",
    "#             c[el.tag] += 1\n",
    "#     total = float(sum([c[ind] for ind in c]))\n",
    "#     for ind in c:\n",
    "#         c[ind] = c[ind] / total\n",
    "#     return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training features...\n",
      "done extracting training features\n"
     ]
    }
   ],
   "source": [
    "# Name the directories\n",
    "train_dir = \"../malware_data/train\"\n",
    "test_dir = \"../malware_data/test\"\n",
    "# outputfile = \"nn_new_features.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# List of feature functions defined above\n",
    "ffs = [tag_count_feats, attrib_feats]\n",
    "\n",
    "# Extract train features\n",
    "print \"extracting training features...\"\n",
    "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print \"done extracting training features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform data into dense array\n",
    "X_train_arr = X_train.toarray()\n",
    "\n",
    "# Get frequencies of each column/feature in original data\n",
    "sums = [(sum(X_train_arr[:,j]),j) for j in xrange(X_train_arr.shape[1])]\n",
    "sums.sort(reverse=True)\n",
    "\n",
    "# Get the 300 pairings with the highest feature frequency\n",
    "top = sums[0:300]\n",
    "inv_map = {v:k for k, v in global_feat_dict.iteritems()}\n",
    "feats = [(inv_map[i],i) for (_,i) in top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features...\n",
      "done extracting test features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # get rid of training data and load test data\n",
    "# # del X_train\n",
    "# # del t_train\n",
    "# # del train_ids\n",
    "# print \"extracting test features...\"\n",
    "# X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "# print \"done extracting test features\"\n",
    "# print\n",
    "\n",
    "# Extract test features\n",
    "print \"extracting test features...\"\n",
    "X_test,test_feat_dict,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print \"done extracting test features\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# Get features for train and test data\n",
    "train_feat_names = [feat[0] for feat in feats]\n",
    "test_feat_names = test_feat_dict.keys()\n",
    "\n",
    "# Get intersection of features\n",
    "inter_feat_names = list(filter(lambda x: x in train_feat_names, test_feat_names))\n",
    "\n",
    "print len(inter_feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only keep features that occur in both train and test data,\n",
    "# so we can train our model properly\n",
    "test_cols = [test_feat_dict[feat] for feat in inter_feat_names]\n",
    "train_cols = [global_feat_dict[feat] for feat in inter_feat_names]\n",
    "X_train_final = X_train[:, train_cols]\n",
    "X_test_final = X_test[:,test_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for k-fold cross validation\n",
    "def kfold(k, model, X, Y):\n",
    "    kf = KFold(n_splits=k)\n",
    "    accs = []\n",
    "    for train_fold_index, validate_fold_index in kf.split(X):\n",
    "        X_train_fold = X[train_fold_index]\n",
    "        X_validate_fold = X[validate_fold_index]\n",
    "        Y_train_fold = Y[train_fold_index]\n",
    "        Y_validate_fold = Y[validate_fold_index]\n",
    "        model.fit(X_train_fold, Y_train_fold)\n",
    "        Y_hat = model.predict(X_validate_fold)\n",
    "        acc = np.mean(Y_hat == Y_validate_fold)\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning...\n",
      "0.744325030291\n",
      "done learning\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "print \"learning...\"\n",
    "Logreg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "Logreg.fit(X_train_final, t_train)\n",
    "print kfold(5, Logreg, X_train_final, t_train)\n",
    "print \"done learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], [0.74529852664264395, 0.74724237226794232, 0.7446491793992227, 0.74432503029063268, 0.74367883012593561, 0.74075991460926394, 0.74691664961999016])\n"
     ]
    }
   ],
   "source": [
    "# tune logreg\n",
    "C_vec = [10. ** a for a in range(-3, 4)]\n",
    "accs = []\n",
    "for C in C_vec:\n",
    "    Logreg = LogisticRegression(C=C, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "    accs.append(kfold(5, Logreg, X_train_final, t_train))\n",
    "print (C_vec, accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0.84478212259519336)\n",
      "(5, 0.83765217436914186)\n",
      "(10, 0.83343315260790996)\n",
      "(20, 0.82760578131545892)\n",
      "(30, 0.81627641347801261)\n",
      "(40, 0.79617049617049618)\n",
      "(50, 0.79095716552088846)\n",
      "(75, 0.77707317073170723)\n",
      "(100, 0.7355806451612904)\n",
      "(150, 0.732079365079365)\n",
      "[(3, 0.84478212259519336), (5, 0.83765217436914186), (10, 0.83343315260790996), (20, 0.82760578131545892), (30, 0.81627641347801261), (40, 0.79617049617049618), (50, 0.79095716552088846), (75, 0.77707317073170723), (100, 0.7355806451612904), (150, 0.732079365079365)]\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "k_values= [3, 5, 10, 20, 30, 40, 50, 75, 100, 150]\n",
    "knn_scores = []\n",
    "X_std = preprocessing.scale(X_train_final.toarray())\n",
    "for k in k_values:\n",
    "    knn = KNN(n_neighbors=k)\n",
    "    result = (k, kfold(k, knn, X_train_final, t_train))\n",
    "    # result = (k, kfold(k, knn, X_std, t_train)) # standardized\n",
    "    print result\n",
    "    knn_scores.append(result)\n",
    "print knn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785160474789\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "svc = SVC()\n",
    "print kfold(5, svc, X_train_final, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682754533105\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "print kfold(5, svc, X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.526348392105\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "print kfold(5, mnb, X_train_final, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884637010695\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "rf = RandomForestClassifier()\n",
    "print kfold(5, rf, X_train_final, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 0.88690605445495208)\n",
      "(10, 3, 0.88139551960892304)\n",
      "(10, 3, 0.87686162819362923)\n",
      "(10, 9, 0.8839902860170048)\n",
      "(10, 9, 0.88107399306593659)\n",
      "(10, 9, 0.87848237373657911)\n",
      "(10, 30, 0.88366666142153538)\n",
      "(10, 30, 0.88722810551105946)\n",
      "(10, 30, 0.88463596166858127)\n",
      "(10, 95, 0.89079584375803178)\n",
      "(10, 95, 0.88820160186307073)\n",
      "(10, 95, 0.88561155607307518)\n",
      "(10, 300, 0.88496483139525739)\n",
      "(10, 300, 0.88366351434281132)\n",
      "(10, 300, 0.88787797726760131)\n",
      "(20, 3, 0.8898223474060204)\n",
      "(20, 3, 0.8907968927842731)\n",
      "(20, 3, 0.88237163852653777)\n",
      "(20, 9, 0.89111894384038026)\n",
      "(20, 9, 0.88561208058619589)\n",
      "(20, 9, 0.88755330364589058)\n",
      "(20, 30, 0.89176619303131877)\n",
      "(20, 30, 0.89144256843584946)\n",
      "(20, 30, 0.8878811243463256)\n",
      "(20, 95, 0.88690500542871076)\n",
      "(20, 95, 0.89532868614708394)\n",
      "(20, 95, 0.88884937556712984)\n",
      "(20, 300, 0.88820317540243265)\n",
      "(20, 300, 0.88787640372823928)\n",
      "(20, 300, 0.8888514736196127)\n",
      "(30, 3, 0.89144519100145292)\n",
      "(30, 3, 0.88949819829743038)\n",
      "(30, 3, 0.88042359679627391)\n",
      "(30, 9, 0.89306383849191984)\n",
      "(30, 9, 0.89111841932725944)\n",
      "(30, 9, 0.8872302035635421)\n",
      "(30, 30, 0.89111894384038026)\n",
      "(30, 30, 0.89371213670909988)\n",
      "(30, 30, 0.89047221916256236)\n",
      "(30, 95, 0.893385889548027)\n",
      "(30, 95, 0.89921847545016342)\n",
      "(30, 95, 0.89338851211363068)\n",
      "(30, 300, 0.89111841932725944)\n",
      "(30, 300, 0.89435991041315899)\n",
      "(30, 300, 0.8894987228105512)\n",
      "(40, 3, 0.89079374570554892)\n",
      "(40, 3, 0.88690710348119361)\n",
      "(40, 3, 0.88723072807666292)\n",
      "(40, 9, 0.89306383849191984)\n",
      "(40, 9, 0.89273968938332993)\n",
      "(40, 9, 0.88495958626405058)\n",
      "(40, 30, 0.89079584375803156)\n",
      "(40, 30, 0.89209139116615011)\n",
      "(40, 30, 0.88852732451102268)\n",
      "(40, 95, 0.89597960692986744)\n",
      "(40, 95, 0.8953307841995668)\n",
      "(40, 95, 0.89241606478786062)\n",
      "(40, 300, 0.89111737030101812)\n",
      "(40, 300, 0.89241344222225716)\n",
      "(40, 300, 0.8869065789680729)\n",
      "(50, 3, 0.88949819829743038)\n",
      "(50, 3, 0.88982496997162386)\n",
      "(50, 3, 0.88561312961243721)\n",
      "(50, 9, 0.8878774527544806)\n",
      "(50, 9, 0.89241606478786062)\n",
      "(50, 9, 0.88593360712918234)\n",
      "(50, 30, 0.89435833687379684)\n",
      "(50, 30, 0.89824602812439358)\n",
      "(50, 30, 0.88625618269841022)\n",
      "(50, 95, 0.89565545782127742)\n",
      "(50, 95, 0.89889589988093554)\n",
      "(50, 95, 0.8930622649525578)\n",
      "(50, 300, 0.8930622649525578)\n",
      "(50, 300, 0.89047012111007962)\n",
      "(50, 300, 0.89079479473179024)\n",
      "(100, 3, 0.89047274367568308)\n",
      "(100, 3, 0.88852679999790196)\n",
      "(100, 3, 0.8865834788857242)\n",
      "(100, 9, 0.89079374570554892)\n",
      "(100, 9, 0.8914441419752116)\n",
      "(100, 9, 0.88593413164230306)\n",
      "(100, 30, 0.89273706681772647)\n",
      "(100, 30, 0.89468248598238687)\n",
      "(100, 30, 0.89047169464944154)\n",
      "(100, 95, 0.89500611057785606)\n",
      "(100, 95, 0.89759982795969651)\n",
      "(100, 95, 0.8930617404394372)\n",
      "(100, 300, 0.88982182289289968)\n",
      "(100, 300, 0.89532921066020454)\n",
      "(100, 300, 0.88884937556712984)\n",
      "[(10, 3, 0.88690605445495208), (10, 3, 0.88139551960892304), (10, 3, 0.87686162819362923), (10, 9, 0.8839902860170048), (10, 9, 0.88107399306593659), (10, 9, 0.87848237373657911), (10, 30, 0.88366666142153538), (10, 30, 0.88722810551105946), (10, 30, 0.88463596166858127), (10, 95, 0.89079584375803178), (10, 95, 0.88820160186307073), (10, 95, 0.88561155607307518), (10, 300, 0.88496483139525739), (10, 300, 0.88366351434281132), (10, 300, 0.88787797726760131), (20, 3, 0.8898223474060204), (20, 3, 0.8907968927842731), (20, 3, 0.88237163852653777), (20, 9, 0.89111894384038026), (20, 9, 0.88561208058619589), (20, 9, 0.88755330364589058), (20, 30, 0.89176619303131877), (20, 30, 0.89144256843584946), (20, 30, 0.8878811243463256), (20, 95, 0.88690500542871076), (20, 95, 0.89532868614708394), (20, 95, 0.88884937556712984), (20, 300, 0.88820317540243265), (20, 300, 0.88787640372823928), (20, 300, 0.8888514736196127), (30, 3, 0.89144519100145292), (30, 3, 0.88949819829743038), (30, 3, 0.88042359679627391), (30, 9, 0.89306383849191984), (30, 9, 0.89111841932725944), (30, 9, 0.8872302035635421), (30, 30, 0.89111894384038026), (30, 30, 0.89371213670909988), (30, 30, 0.89047221916256236), (30, 95, 0.893385889548027), (30, 95, 0.89921847545016342), (30, 95, 0.89338851211363068), (30, 300, 0.89111841932725944), (30, 300, 0.89435991041315899), (30, 300, 0.8894987228105512), (40, 3, 0.89079374570554892), (40, 3, 0.88690710348119361), (40, 3, 0.88723072807666292), (40, 9, 0.89306383849191984), (40, 9, 0.89273968938332993), (40, 9, 0.88495958626405058), (40, 30, 0.89079584375803156), (40, 30, 0.89209139116615011), (40, 30, 0.88852732451102268), (40, 95, 0.89597960692986744), (40, 95, 0.8953307841995668), (40, 95, 0.89241606478786062), (40, 300, 0.89111737030101812), (40, 300, 0.89241344222225716), (40, 300, 0.8869065789680729), (50, 3, 0.88949819829743038), (50, 3, 0.88982496997162386), (50, 3, 0.88561312961243721), (50, 9, 0.8878774527544806), (50, 9, 0.89241606478786062), (50, 9, 0.88593360712918234), (50, 30, 0.89435833687379684), (50, 30, 0.89824602812439358), (50, 30, 0.88625618269841022), (50, 95, 0.89565545782127742), (50, 95, 0.89889589988093554), (50, 95, 0.8930622649525578), (50, 300, 0.8930622649525578), (50, 300, 0.89047012111007962), (50, 300, 0.89079479473179024), (100, 3, 0.89047274367568308), (100, 3, 0.88852679999790196), (100, 3, 0.8865834788857242), (100, 9, 0.89079374570554892), (100, 9, 0.8914441419752116), (100, 9, 0.88593413164230306), (100, 30, 0.89273706681772647), (100, 30, 0.89468248598238687), (100, 30, 0.89047169464944154), (100, 95, 0.89500611057785606), (100, 95, 0.89759982795969651), (100, 95, 0.8930617404394372), (100, 300, 0.88982182289289968), (100, 300, 0.89532921066020454), (100, 300, 0.88884937556712984)]\n"
     ]
    }
   ],
   "source": [
    "# Tune random forest\n",
    "n_estimators = [10, 20, 30, 40, 50, 100]\n",
    "rf_scores = []\n",
    "max_features = [int(X_train_final.shape[1] ** ((i+1) / 5.)) for i in range(5)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "for n in n_estimators:\n",
    "    for m in max_features:\n",
    "        for p in min_samples_split:\n",
    "            RF = RandomForestClassifier(n_estimators=n, max_features=m, min_samples_split=p)\n",
    "            result = (n, m, kfold(5, RF, X_train_final, t_train))\n",
    "            print result\n",
    "            rf_scores.append(result)\n",
    "print rf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 95, 0.89921847545016342)\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_score = max(rf_scores, key= lambda item:item[2])\n",
    "print best_score\n",
    "# n = 30, p = 5, m = 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n,m = X_train_final.shape\n",
    "num_labels = 15\n",
    "d = 50\n",
    "eta = 0.001\n",
    "num_epochs = 500\n",
    "BATCH_SIZE = 100\n",
    "num_steps = num_epochs * n // BATCH_SIZE\n",
    "\n",
    "# Turn labels into one-hot matrix\n",
    "Y = np.zeros((n,num_labels))\n",
    "for i in range(n):\n",
    "    Y[i,t_train[i]] = 1\n",
    "\n",
    "# Placeholder for x and y_\n",
    "x = tf.placeholder(tf.float32, [None,m])\n",
    "y_ = tf.placeholder(tf.float32, [None,num_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the hidden weights and biases.\n",
    "W_hidden = tf.Variable(tf.random_uniform((m,d),0,0.5))\n",
    "b_hidden = tf.Variable(tf.random_uniform((1,d),0,0.5))\n",
    "\n",
    "# The hidden layer.\n",
    "hidden = tf.nn.softmax(tf.matmul(x,W_hidden) + b_hidden)\n",
    "\n",
    "# Initialize the output weights and biases.\n",
    "W_out = tf.Variable(tf.random_uniform((d,num_labels),0,0.5))\n",
    "b_out = tf.Variable(tf.random_uniform((1,num_labels),0,0.5))\n",
    "\n",
    "# The output layer.\n",
    "y = tf.nn.softmax(tf.matmul(hidden,W_out) + b_out)\n",
    "\n",
    "# Optimization.\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.AdamOptimizer(eta).minimize(cross_entropy) # BEST\n",
    "# train_step = tf.train.MomentumOptimizer(eta,0.3).minimize(cross_entropy)\n",
    "# train_step = tf.train.AdagradOptimizer(eta).minimize(cross_entropy)\n",
    "# train_step = tf.train.GradientDescentOptimizer(eta).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation.\n",
    "predicted_class = tf.argmax(y,1)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n",
      "training...\n",
      "Step 0/15430\n",
      "Step 1000/15430\n",
      "Step 2000/15430\n",
      "Step 3000/15430\n",
      "Step 4000/15430\n",
      "Step 5000/15430\n",
      "Step 6000/15430\n",
      "Step 7000/15430\n",
      "Step 8000/15430\n",
      "Step 9000/15430\n",
      "Step 10000/15430\n",
      "Step 11000/15430\n",
      "Step 12000/15430\n",
      "Step 13000/15430\n",
      "Step 14000/15430\n",
      "Step 15000/15430\n",
      "Test Accuracy: 0.846727\n",
      "making predictions...\n",
      "done making predictions\n",
      "\n",
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Create a local session to run this computation.\n",
    "with tf.Session() as s:\n",
    "    print \"initializing...\"\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    print \"training...\"\n",
    "    # Iterate and train.\n",
    "    for step in xrange(num_steps):\n",
    "        if step % 1000 == 0:\n",
    "            print \"Step %d/%d\" % (step, num_steps)\n",
    "        offset = (step * BATCH_SIZE) % n\n",
    "        batch_data = X_train_final.toarray()[offset:(offset + BATCH_SIZE), :]\n",
    "        batch_labels = Y[offset:(offset + BATCH_SIZE)]\n",
    "        train_step.run(feed_dict={x: batch_data, y_: batch_labels})\n",
    "\n",
    "    print \"Test Accuracy:\", accuracy.eval(feed_dict={x: X_train_final.toarray(), y_: Y})\n",
    "    \n",
    "    print \"making predictions...\"\n",
    "    classification = s.run(tf.argmax(y,1), feed_dict={x: X_test_final.toarray()})\n",
    "    print \"done making predictions\"\n",
    "    print\n",
    "\n",
    "    print \"writing predictions...\"\n",
    "    util.write_predictions(classification, test_ids, \"nn_new_features.csv\")\n",
    "    print \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions...\n",
      "done making predictions\n",
      "\n",
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "print \"making predictions...\"\n",
    "rf = RandomForestClassifier(n_estimators=30, max_features=95, min_samples_split=5)\n",
    "rf.fit(X_train_final,t_train)\n",
    "preds = rf.predict(X_test_final)\n",
    "print \"done making predictions\"\n",
    "print\n",
    "\n",
    "# Write predictions to specified output file\n",
    "print \"writing predictions...\"\n",
    "util.write_predictions(preds, test_ids, \"rf_newfeatures.csv\")\n",
    "print \"done!\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
