{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This file provides starter code for extracting features from the xml files and\n",
    "## for doing some learning.\n",
    "##\n",
    "## The basic set-up: \n",
    "## ----------------\n",
    "## main() will run code to extract features, learn, and make predictions.\n",
    "## \n",
    "## extract_feats() is called by main(), and it will iterate through the \n",
    "## train/test directories and parse each xml file into an xml.etree.ElementTree, \n",
    "## which is a standard python object used to represent an xml file in memory.\n",
    "## (More information about xml.etree.ElementTree objects can be found here:\n",
    "## http://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "## and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)\n",
    "## It will then use a series of \"feature-functions\" that you will write/modify\n",
    "## in order to extract dictionaries of features from each ElementTree object.\n",
    "## Finally, it will produce an N x D sparse design matrix containing the union\n",
    "## of the features contained in the dictionaries produced by your \"feature-functions.\"\n",
    "## This matrix can then be plugged into your learning algorithm.\n",
    "##\n",
    "## The learning and prediction parts of main() are largely left to you, though\n",
    "## it does contain code that randomly picks class-specific weights and predicts\n",
    "## the class with the weights that give the highest score. If your prediction\n",
    "## algorithm involves class-specific weights, you should, of course, learn \n",
    "## these class-specific weights in a more intelligent way.\n",
    "##\n",
    "## Feature-functions:\n",
    "## --------------------\n",
    "## \"feature-functions\" are functions that take an ElementTree object representing\n",
    "## an xml file (which contains, among other things, the sequence of system calls a\n",
    "## piece of potential malware has made), and returns a dictionary mapping feature names to \n",
    "## their respective numeric values. \n",
    "## For instance, a simple feature-function might map a system call history to the\n",
    "## dictionary {'first_call-load_image': 1}. This is a boolean feature indicating\n",
    "## whether the first system call made by the executable was 'load_image'. \n",
    "## Real-valued or count-based features can of course also be defined in this way. \n",
    "## Because this feature-function will be run over ElementTree objects for each \n",
    "## software execution history instance, we will have the (different)\n",
    "## feature values of this feature for each history, and these values will make up \n",
    "## one of the columns in our final design matrix.\n",
    "## Of course, multiple features can be defined within a single dictionary, and in\n",
    "## the end all the dictionaries returned by feature functions (for a particular\n",
    "## training example) will be unioned, so we can collect all the feature values \n",
    "## associated with that particular instance.\n",
    "##\n",
    "## Two example feature-functions, first_last_system_call_feats() and \n",
    "## system_call_count_feats(), are defined below.\n",
    "## The first of these functions indicates what the first and last system-calls \n",
    "## made by an executable are, and the second records the total number of system\n",
    "## calls made by an executable.\n",
    "##\n",
    "## What you need to do:\n",
    "## --------------------\n",
    "## 1. Write new feature-functions (or modify the example feature-functions) to\n",
    "## extract useful features for this prediction task.\n",
    "## 2. Implement an algorithm to learn from the design matrix produced, and to\n",
    "## make predictions on unseen data. Naive code for these two steps is provided\n",
    "## below, and marked by TODOs.\n",
    "##\n",
    "## Computational Caveat\n",
    "## --------------------\n",
    "## Because the biggest of any of the xml files is only around 35MB, the code below \n",
    "## will parse an entire xml file and store it in memory, compute features, and\n",
    "## then get rid of it before parsing the next one. Storing the biggest of the files \n",
    "## in memory should require at most 200MB or so, which should be no problem for\n",
    "## reasonably modern laptops. If this is too much, however, you can lower the\n",
    "## memory requirement by using ElementTree.iterparse(), which does parsing in\n",
    "## a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\n",
    "## for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras neural net imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.convolutional import Convolution1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "    returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    fds = [] # list of feature dicts\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        # accumulate features\n",
    "        [rowfd.update(ff(tree)) for ff in ffs]\n",
    "        fds.append(rowfd)\n",
    "        \n",
    "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "    return X, feat_dict, np.array(classes), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      fds is a list of feature dicts (one for each row).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "       \n",
    "    returns: \n",
    "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "        the union of features defined in any of the fds \n",
    "    \"\"\"\n",
    "    if global_feat_dict is None:\n",
    "        all_feats = set()\n",
    "        [all_feats.update(fd.keys()) for fd in fds]\n",
    "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "    else:\n",
    "        feat_dict = global_feat_dict\n",
    "        \n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []        \n",
    "    for i in xrange(len(fds)):\n",
    "        temp_cols = []\n",
    "        temp_data = []\n",
    "        for feat,val in fds[i].iteritems():\n",
    "            try:\n",
    "                # update temp_cols iff update temp_data\n",
    "                temp_cols.append(feat_dict[feat])\n",
    "                temp_data.append(val)\n",
    "            except KeyError as ex:\n",
    "                if global_feat_dict is not None:\n",
    "                    pass  # new feature in test data; nbd\n",
    "                else:\n",
    "                    raise ex\n",
    "\n",
    "        # all fd's features in the same row\n",
    "        k = len(temp_cols)\n",
    "        cols.extend(temp_cols)\n",
    "        data.extend(temp_data)\n",
    "        rows.extend([i]*k)\n",
    "\n",
    "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "\n",
    "    X = sparse.csr_matrix((np.array(data),\n",
    "                   (np.array(rows), np.array(cols))),\n",
    "                   shape=(len(fds), len(feat_dict)))\n",
    "    return X, feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# feature-names to numeric values.\n",
    "## TODO: modify these functions, and/or add new ones.\n",
    "def first_last_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "      (in other words, it returns a dictionary indicating what the first and \n",
    "      last system calls made by an executable were.)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    first = True # is this the first system call\n",
    "    last_call = None # keep track of last call we've seen\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            if first:\n",
    "                c[\"first_call-\"+el.tag] = 1\n",
    "                first = False\n",
    "            last_call = el.tag  # update last call seen\n",
    "            \n",
    "    # finally, mark last call seen\n",
    "    c[\"last_call-\"+last_call] = 1\n",
    "    return c\n",
    "\n",
    "def system_call_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c['num_system_calls'] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features based on name of system calls\n",
    "def tag_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping el.tag to the number of system_calls\n",
    "      identified by that tag (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features based on system call proportions\n",
    "def tag_prop_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping el.tag to the number of system_calls\n",
    "      identified by that tag (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag] += 1\n",
    "    total = float(sum([c[ind] for ind in c]))\n",
    "    for ind in c:\n",
    "        c[ind] = c[ind] / total\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for k-fold cross validation\n",
    "def kfold(k, model, X, Y):\n",
    "    kf = KFold(n_splits=k)\n",
    "    accs = []\n",
    "    for train_fold_index, validate_fold_index in kf.split(X):\n",
    "        X_train_fold = X[train_fold_index]\n",
    "        X_validate_fold = X[validate_fold_index]\n",
    "        Y_train_fold = Y[train_fold_index]\n",
    "        Y_validate_fold = Y[validate_fold_index]\n",
    "        model.fit(X_train_fold, Y_train_fold)\n",
    "        Y_hat = model.predict(X_validate_fold)\n",
    "        acc = np.mean(Y_hat == Y_validate_fold)\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training features...\n",
      "done extracting training features\n"
     ]
    }
   ],
   "source": [
    "## The following function does the feature extraction, learning, and prediction\n",
    "train_dir = \"../malware_data/train\"\n",
    "test_dir = \"../malware_data/test\"\n",
    "outputfile = \"mypredictions.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# TODO put the names of the feature functions you've defined above in this list\n",
    "# ffs = [first_last_system_call_feats, system_call_count_feats, tag_count_feats]\n",
    "ffs = [tag_count_feats]\n",
    "\n",
    "# extract features\n",
    "print \"extracting training features...\"\n",
    "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print \"done extracting training features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning...\n",
      "0.732663530078\n",
      "done learning\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "print \"learning...\"\n",
    "Logreg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "Logreg.fit(X_train, t_train)\n",
    "print kfold(5, Logreg, X_train, t_train)\n",
    "print \"done learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], [0.72262172638248545, 0.7171153876414218, 0.71354922293381162, 0.7203511090829936, 0.71063345449586424, 0.72100255437889782, 0.71614084226316921])\n"
     ]
    }
   ],
   "source": [
    "# tune logreg\n",
    "C_vec = [10. ** a for a in range(-3, 4)]\n",
    "accs = []\n",
    "for C in C_vec:\n",
    "    Logreg = LogisticRegression(C=C, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "    accs.append(kfold(5, Logreg, X_train, t_train))\n",
    "print (C_vec, accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782892480055\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "svc = SVC()\n",
    "print kfold(5, svc, X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682754533105\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "print kfold(5, svc, X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.461850587192\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "mnb = MultinomialNB(fit_prior=False)\n",
    "print kfold(5, mnb, X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 16, 40, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884962733343\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "rf = RandomForestClassifier()\n",
    "print kfold(5, rf, X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2, 0.88366246531656978)\n",
      "(10, 6, 0.88722967905042138)\n",
      "(10, 16, 0.88042202325691188)\n",
      "(10, 40, 0.88690762799431422)\n",
      "(10, 102, 0.88269211604328279)\n",
      "(20, 2, 0.88755645072461498)\n",
      "(20, 6, 0.89111789481413894)\n",
      "(20, 16, 0.88982339643226172)\n",
      "(20, 40, 0.88917457370196118)\n",
      "(20, 102, 0.89014702102773102)\n",
      "(30, 2, 0.88788007532008406)\n",
      "(30, 6, 0.89046802305759676)\n",
      "(30, 16, 0.8875538281590114)\n",
      "(30, 40, 0.88496273334277453)\n",
      "(30, 102, 0.88723282612914556)\n",
      "(40, 2, 0.8914441419752116)\n",
      "(40, 6, 0.88917509821508178)\n",
      "(40, 16, 0.8911168457878974)\n",
      "(40, 40, 0.8891756227282025)\n",
      "(40, 102, 0.89111946835350087)\n",
      "(50, 2, 0.89209086665302928)\n",
      "(50, 6, 0.88982339643226172)\n",
      "(50, 16, 0.89144361746209078)\n",
      "(50, 40, 0.88885042459337116)\n",
      "(50, 102, 0.88690500542871076)\n",
      "(100, 2, 0.89144519100145292)\n",
      "(100, 6, 0.89144256843584946)\n",
      "(100, 16, 0.88949767378430966)\n",
      "(100, 40, 0.89079531924491095)\n",
      "(100, 102, 0.89176566851819794)\n",
      "[(10, 2, 0.88366246531656978), (10, 6, 0.88722967905042138), (10, 16, 0.88042202325691188), (10, 40, 0.88690762799431422), (10, 102, 0.88269211604328279), (20, 2, 0.88755645072461498), (20, 6, 0.89111789481413894), (20, 16, 0.88982339643226172), (20, 40, 0.88917457370196118), (20, 102, 0.89014702102773102), (30, 2, 0.88788007532008406), (30, 6, 0.89046802305759676), (30, 16, 0.8875538281590114), (30, 40, 0.88496273334277453), (30, 102, 0.88723282612914556), (40, 2, 0.8914441419752116), (40, 6, 0.88917509821508178), (40, 16, 0.8911168457878974), (40, 40, 0.8891756227282025), (40, 102, 0.89111946835350087), (50, 2, 0.89209086665302928), (50, 6, 0.88982339643226172), (50, 16, 0.89144361746209078), (50, 40, 0.88885042459337116), (50, 102, 0.88690500542871076), (100, 2, 0.89144519100145292), (100, 6, 0.89144256843584946), (100, 16, 0.88949767378430966), (100, 40, 0.89079531924491095), (100, 102, 0.89176566851819794)]\n"
     ]
    }
   ],
   "source": [
    "# Tune random forest\n",
    "n_estimators = [10, 20, 30, 40, 50, 100]\n",
    "rf_scores = []\n",
    "max_features = [int(X_train.shape[1] ** ((i+1) / 5.)) for i in range(5)]\n",
    "for n in n_estimators:\n",
    "    for m in max_features:\n",
    "        RF = RandomForestClassifier(n_estimators=n)\n",
    "        result = (n, m, kfold(5, RF, X_train, t_train))\n",
    "        print result\n",
    "        rf_scores.append(result)\n",
    "print rf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860982517978\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn = KNN()\n",
    "X_std = preprocessing.scale(X_train.toarray())\n",
    "print kfold(5, knn, X_std, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0.85645117784004476)\n",
      "(5, 0.8609825179776871)\n",
      "(10, 0.84965325936199732)\n",
      "(20, 0.84055509007121931)\n",
      "(30, 0.8273113776254839)\n",
      "(40, 0.82210289710289719)\n",
      "(50, 0.81499735589635125)\n",
      "(75, 0.77965156794425094)\n",
      "(100, 0.76319354838709674)\n",
      "(150, 0.71658730158730155)\n",
      "[(3, 0.85645117784004476), (5, 0.8609825179776871), (10, 0.84965325936199732), (20, 0.84055509007121931), (30, 0.8273113776254839), (40, 0.82210289710289719), (50, 0.81499735589635125), (75, 0.77965156794425094), (100, 0.76319354838709674), (150, 0.71658730158730155)]\n"
     ]
    }
   ],
   "source": [
    "k_values= [3, 5, 10, 20, 30, 40, 50, 75, 100, 150]\n",
    "knn_scores = []\n",
    "for k in k_values:\n",
    "    knn = KNN(n_neighbors=k)\n",
    "    result = (k, kfold(k, knn, X_std, t_train))\n",
    "    print result\n",
    "    knn_scores.append(result)\n",
    "print knn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural net\n",
    "t_train_cat = np_utils.to_categorical(t_train) # one hot encode the class\n",
    "\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=102, init='normal', activation='relu'))\n",
    "    model.add(Dense(15, init='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn=nn_model, nb_epoch=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_arr = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3086L, 102L)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri_000\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:12: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(5, classifier, X_train_arr, t_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KFold_5 = KFold(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\harri_000\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.12-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 84.35% (1.89%)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(classifier, X_train_arr, t_train_cat, cv=KFold_5)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3086L, 15L)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "kfold() takes exactly 4 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8de976affe4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: kfold() takes exactly 4 arguments (3 given)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features...\n",
      "done extracting test features\n"
     ]
    }
   ],
   "source": [
    "# get rid of training data and load test data\n",
    "# del X_train\n",
    "# del t_train\n",
    "# del train_ids\n",
    "print \"extracting test features...\"\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print \"done extracting test features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions...\n",
      "done making predictions\n",
      "\n",
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# TODO make predictions on text data and write them out\n",
    "print \"making predictions...\"\n",
    "# rf = RandomForestClassifier(n_estimators=50, max_features=2)\n",
    "# rf.fit(X_train, t_train)\n",
    "# preds = np.argmax(X_test.dot(learned_W),axis=1)\n",
    "# preds = rf.predict(X_test)\n",
    "# preds = classifier.predict(X_test.toarray())\n",
    "knn = KNN()\n",
    "knn.fit(X_train, t_train)\n",
    "preds = knn.predict(X_test)\n",
    "print \"done making predictions\"\n",
    "print\n",
    "\n",
    "print \"writing predictions...\"\n",
    "util.write_predictions(preds, test_ids, outputfile)\n",
    "print \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
